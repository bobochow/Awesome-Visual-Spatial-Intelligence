# Multimodal Alignment

## Related Resource

- [Awesome-Multimodal-Large-Language-Models](https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models) ![Stars](https://img.shields.io/github/stars/BradyFU/Awesome-Multimodal-Large-Language-Models?style=social) ![Last Commit](https://img.shields.io/github/last-commit/BradyFU/Awesome-Multimodal-Large-Language-Models)

- [Awesome-Multimodal-Research](https://github.com/Eurus-Holmes/Awesome-Multimodal-Research) ![Stars](https://img.shields.io/github/stars/Eurus-Holmes/Awesome-Multimodal-Research?style=social) ![Last Commit](https://img.shields.io/github/last-commit/Eurus-Holmes/Awesome-Multimodal-Research)

- [Awesome-Multimodal-ML](https://github.com/pliang279/awesome-multimodal-ml) ![Stars](https://img.shields.io/github/stars/pliang279/awesome-multimodal-ml?style=social) ![Last Commit](https://img.shields.io/github/last-commit/pliang279/awesome-multimodal-ml)

- [Awesome-RL-based-Reasoning-MLLMs](https://github.com/Sun-Haoyuan23/Awesome-RL-based-Reasoning-MLLMs) ![Stars](https://img.shields.io/github/stars/Sun-Haoyuan23/Awesome-RL-based-Reasoning-MLLMs?style=social) ![Last Commit](https://img.shields.io/github/last-commit/Sun-Haoyuan23/Awesome-RL-based-Reasoning-MLLMs)

## Survey

- Aligning Multimodal LLM with Human Preference: A Survey [[Paper](https://arxiv.org/abs/2503.14504)] ![Static Badge](https://img.shields.io/badge/arXiv%202503-red)

## Multimodal Alignment with Text, Image, Audio, Depth, Video
- PyramidCLIP: Hierarchical Feature Alignment for Vision-language Model Pretraining [[Paper](https://arxiv.org/abs/2204.14095)] ![Static Badge](https://img.shields.io/badge/NeurIPS%202022-blue)
- Audioclip: Extending Clip to Image, Text and Audio [[Paper](https://doi.org/10.1109/ICASSP43922.2022.9747631)] ![Static Badge](https://img.shields.io/badge/ICASSP%202022-blue)
- ImageBind: One Embedding Space to Bind Them All [[Paper](https://arxiv.org/abs/2305.01877)] ![Static Badge](https://img.shields.io/badge/CVPR%202023-blue)
- ONE-PEACE: Exploring One General Representation Model Toward Unlimited Modalities [[Paper](https://arxiv.org/abs/2305.11172)] ![Static Badge](https://img.shields.io/badge/arXiv%202305-red)
- X²-VLM: All-in-One Pre-Trained Model for Vision-Language Tasks [[Paper](https://doi.org/10.1109/TPAMI.2023.3339661)] ![Static Badge](https://img.shields.io/badge/TPAMI-green)
- ImageBind-LLM: Multi-modality Instruction Tuning. [[Paper](https://arxiv.org/abs/2309.03905)] ![Static Badge](https://img.shields.io/badge/arXiv%202309-red)
- LanguageBind: Extending Video-Language Pretraining to N-modality by Language-based Semantic Alignment. [[Paper](https://arxiv.org/abs/2310.01852)] ![Static Badge](https://img.shields.io/badge/iclr%202024-blue)
- UniBind: LLM-Augmented Unified and Balanced Representation Space to Bind Them All. [[Paper](https://arxiv.org/abs/2403.12532)] ![Static Badge](https://img.shields.io/badge/CVPR%202024-blue)
- Implicit Multimodal Alignment: On the Generalization of Frozen LLMs to Multimodal Inputs [[Paper](https://arxiv.org/abs/2405.16700)] ![Static Badge](https://img.shields.io/badge/NeurIPS%202024-blue)
- OmniBind: Teach to Build Unequal-Scale Modality Interaction for Omni-Bind of All. [[Paper](https://arxiv.org/abs/2405.16108)] ![Static Badge](https://img.shields.io/badge/arXiv%202405-red)
- FreeBind: Free Lunch in Unified Multimodal Space via Knowledge Fusion. [[Paper](https://arxiv.org/abs/2405.04883)] ![Static Badge](https://img.shields.io/badge/ICML%202024-blue)
- OmniBind: Large-scale Omni Multimodal Representation via Binding Spaces. [[Paper](https://arxiv.org/abs/2407.11895)] ![Static Badge](https://img.shields.io/badge/ICLR%202025-blue)
- Alt-MoE: Multimodal Alignment via Alternating Optimization of Multi-directional MoE with Unimodal Models [[Paper](https://arxiv.org/abs/2409.05929)] ![Static Badge](https://img.shields.io/badge/arXiv%202409-red)
- BIP3D: Bridging 2D Images and 3D Perception for Embodied Intelligence. [[Paper](https://arxiv.org/abs/2411.14869)]![Static Badge](https://img.shields.io/badge/arxiv%202411-red)
- Assessing and Learning Alignment of Unimodal Vision and Language Models [[Paper](https://arxiv.org/abs/2412.04616)] ![Static Badge](https://img.shields.io/badge/arXiv%202412-red)
- AlignMamba: Enhancing Multimodal Mamba with Local and Global Cross-modal Alignment [[Paper](https://arxiv.org/abs/2412.00833)] ![Static Badge](https://img.shields.io/badge/arXiv%202412-red)
- Context-Based Semantic-Aware Alignment for Semi-Supervised Multi-Label Learning [[Paper](https://arxiv.org/abs/2412.18842)] ![Static Badge](https://img.shields.io/badge/arXiv%202412-red)
- Cross the Gap: Exposing the Intra-modal Misalignment in CLIP via Modality Inversion [[Paper](https://arxiv.org/abs/2502.04263)] ![Static Badge](https://img.shields.io/badge/ICLR%202025-blue)
- Universal Sparse Autoencoders: Interpretable Cross-Model Concept Alignment [[Paper](https://arxiv.org/abs/2502.03714)] ![Static Badge](https://img.shields.io/badge/arXiv%202502-red)
- TULIP: Towards Unified Language-Image Pretraining [[Paper](https://arxiv.org/abs/2503.15485)] ![Static Badge](https://img.shields.io/badge/CVPR%202025-blue)
- ......


## Multimodal Alignment with Point Cloud

- CLIP2Point: Transfer CLIP to Point Cloud Classification with Image-Depth Pre-Training. [[Paper](https://arxiv.org/abs/2210.01055)]![Static Badge](https://img.shields.io/badge/ICCV%202023-blue)

- Autoencoders as Cross-Modal Teachers: Can Pretrained 2D Image Transformers Help 3D Representation Learning? [[Paper](https://arxiv.org/abs/2212.08320)]![Static Badge](https://img.shields.io/badge/ICLR%202023-blue)

- PointCLIP V2: Prompting CLIP and GPT for Powerful 3D Open-world Learning. [[Paper](https://arxiv.org/abs/2211.11682)]![Static Badge](https://img.shields.io/badge/ICCV%202023-blue)

- ULIP: Learning a Unified Representation of Language, Images, and Point Clouds for 3D Understanding. [[Paper](https://arxiv.org/abs/2212.05171)]![Static Badge](https://img.shields.io/badge/CVPR%202023-blue)

- PLA: Language-Driven Open-Vocabulary 3D Scene Understanding. [[Paper](https://arxiv.org/abs/2211.16312)]![Static Badge](https://img.shields.io/badge/CVPR%202023-blue)

- Contrast with Reconstruct: Contrastive 3D Representation Learning Guided by Generative Pretraining. [[Paper](https://arxiv.org/abs/2302.02318)]![Static Badge](https://img.shields.io/badge/ICML%202023-blue)

- CLIP-FO3D: Learning Free Open-world 3D Scene Representations from 2D Dense CLIP. [[Paper](https://arxiv.org/abs/2303.04748)]![Static Badge](https://img.shields.io/badge/ICCVW%202023-blue)

- OpenShape: Scaling Up 3D Shape Representation Towards Open-World Understanding. [[Paper](https://arxiv.org/abs/2305.10764)]![Static Badge](https://img.shields.io/badge/NeurIPS%202023-blue)

- ULIP-2: Towards Scalable Multimodal Pre-training for 3D Understanding. [[Paper](https://arxiv.org/abs/2305.08275)]![Static Badge](https://img.shields.io/badge/CVPR%202024-blue)

- Meta-Transformer: A Unified Framework for Multimodal Learning. [[Paper](https://arxiv.org/abs/2307.10802)]![Static Badge](https://img.shields.io/badge/arxiv%202307-red)

- OneLLM: One Framework to Align All Modalities with Language. [[Paper](https://arxiv.org/abs/2312.03700)]![Static Badge](https://img.shields.io/badge/CVPR%202024-blue)

- Point-Bind & Point-LLM: Aligning Point Cloud with Multi-modality for 3D Understanding, Generation, and Instruction Following. [[Paper](https://arxiv.org/abs/2309.00615)]![Static Badge](https://img.shields.io/badge/arxiv%202309-red)

- Uni3D: Exploring Unified 3D Representation at Scale. [[Paper](https://arxiv.org/abs/2310.06773)]![Static Badge](https://img.shields.io/badge/ICLR%202024-blue)

- Extending Multi-modal Contrastive Representations. [[Paper](https://arxiv.org/abs/2310.08884)]![Static Badge](https://img.shields.io/badge/arxiv%202310-red)

- ViT-Lens: Towards Omni-modal Representations. [[Paper](https://arxiv.org/abs/2311.16081)]![Static Badge](https://img.shields.io/badge/CVPR%202024-blue)

- Multimodal Pathway: Improve Transformers with Irrelevant Data from Other Modalities. [[Paper](https://arxiv.org/abs/2401.14405)]![Static Badge](https://img.shields.io/badge/CVPR%202024-blue)

- Physical Property Understanding from Language-Embedded Feature Fields. [[Paper](https://arxiv.org/abs/2404.04242)]![Static Badge](https://img.shields.io/badge/CVPR%202024-blue)

- Explore the Limits of Omni-modal Pretraining at Scale. [[Paper](https://arxiv.org/abs/2406.09412)]![Static Badge](https://img.shields.io/badge/arxiv%202406-red)


- Dense Multimodal Alignment for Open-Vocabulary 3D Scene Understanding. [[Paper](https://arxiv.org/abs/2407.09781)]![Static Badge](https://img.shields.io/badge/ECCV%202024-blue)

- Open Vocabulary 3D Scene Understanding via Geometry Guided Self-Distillation. [[Paper](https://arxiv.org/abs/2407.13362)]![Static Badge](https://img.shields.io/badge/ECCV%202024-blue)

- CLIP-GS: Unifying Vision-Language Representation with 3D Gaussian Splatting. [[Paper](https://arxiv.org/abs/2412.19142)]![Static Badge](https://img.shields.io/badge/arxiv%202412-red)

- UniGS: Unified Language-Image-3D Pretraining with Gaussian Splatting. [[Paper](https://arxiv.org/abs/2502.17860)]![Static Badge](https://img.shields.io/badge/ICLR%202025-blue)

- CrossOver: 3D Scene Cross-Modal Alignment. [[Paper](https://arxiv.org/abs/2502.15011)]![Static Badge](https://img.shields.io/badge/arxiv%202502-red)

- Escaping Plato’s Cave: Towards the Alignment of 3D and Text Latent Spaces. [[Paper](https://arxiv.org/abs/2503.05283)]![Static Badge](https://img.shields.io/badge/arxiv%202503-red)

- ......

## Reinforcement Learning Based Multimodal Alignment

- Align Anything: Training All-Modality Models to Follow Instructions with Language Feedback [[Paper](https://arxiv.org/abs/2412.15838)] ![Static Badge](https://img.shields.io/badge/arXiv%202412-red)
- Task Preference Optimization: Improving Multimodal Large Language Models with Vision Task Alignment [[Paper](https://arxiv.org/abs/2412.19326)] ![Static Badge](https://img.shields.io/badge/arXiv%202412-red)
- Re-Align: Aligning Vision Language Models via Retrieval-Augmented Direct Preference Optimization [[Paper](https://arxiv.org/abs/2502.13146)] ![Static Badge](https://img.shields.io/badge/arXiv%202502-red)
- Unified Reward Model for Multimodal Understanding and Generation. [[Paper](https://arxiv.org/abs/2503.05236)] ![Static Badge](https://img.shields.io/badge/arXiv%202503-red)
- R1-Omni: Explainable Omni-Multimodal Emotion Recognition with Reinforcement Learning [[Paper](https://arxiv.org/abs/2503.05379)] ![Static Badge](https://img.shields.io/badge/arXiv%202503-red)

- ......

